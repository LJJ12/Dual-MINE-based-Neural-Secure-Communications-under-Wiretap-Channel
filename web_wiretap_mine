# -*- coding: utf-8 -*-
#改了true,false
import numpy as np
#% matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import OneHotEncoder
import pandas as pd
from scipy import special
# from Clustering_Equalgrps.equal_groups import EqualGroupsKMeans
from sklearn.cluster import KMeans  # 网页版改动，后面的EqualGroupsKMeans记得修改

np.random.seed(42)
tf.random.set_seed(42)
# %%

M = 16
k = int(np.log2(M))
n = 1
TRAINING_SNR = 7
rayleigh = True

#%%

def EbNo_to_noise(ebnodb):
    '''Transform EbNo[dB]/snr to noise power'''
    ebno = 10**(ebnodb/10)
    noise_std = 1/np.sqrt(2*(k/n)*ebno)
    return noise_std


# Generate Training Data
# x = tf.random.uniform(shape=[SAMPLE_SIZE], minval=0, maxval=M, dtype=tf.int64)
# x_1h = tf.one_hot(x, M)
# dataset = tf.data.Dataset.from_tensor_slices(x_1h)

# %%

def snr_to_noise(snrdb):
    '''Transform snr to noise power'''
    snr = 10 ** (snrdb / 10)
    noise_std = 1 / np.sqrt(2 * snr)  # 1/np.sqrt(2*(k/n)*ebno) for ebno to noise
    return noise_std

class NN_function0(tf.keras.Model):
    def __init__(self, hidden_dim, layers, activation, **extra_kwargs):
        super(NN_function0, self).__init__()
        self._f = tf.keras.Sequential(
          [tf.keras.layers.Dense(hidden_dim, activation) for _ in range(layers)] +
          [tf.keras.layers.Dense(1)])

    def call(self, x, y):
        batch_size = tf.shape(x)[0]
        x_tiled = tf.tile(x[None, :],  (batch_size, 1, 1))
        y_tiled = tf.tile(y[:, None],  (1, batch_size, 1))
        xy_pairs = tf.reshape(tf.concat((x_tiled, y_tiled), axis=2), [batch_size * batch_size, -1])
        scores = self._f(xy_pairs)
        return tf.transpose(tf.reshape(scores, [batch_size, batch_size]))
class NN_function1(tf.keras.Model):
    def __init__(self, hidden_dim, layers, activation, **extra_kwargs):
        super(NN_function1, self).__init__()
        self._f = tf.keras.Sequential(
          [tf.keras.layers.Dense(hidden_dim, activation) for _ in range(layers)] +
          [tf.keras.layers.Dense(1)])

    def call(self, x, y):
        batch_size = tf.shape(x)[0]
        x_tiled = tf.tile(x[None, :],  (batch_size, 1, 1))
        y_tiled = tf.tile(y[:, None],  (1, batch_size, 1))
        xy_pairs = tf.reshape(tf.concat((x_tiled, y_tiled), axis=2), [batch_size * batch_size, -1])
        scores = self._f(xy_pairs)
        return tf.transpose(tf.reshape(scores, [batch_size, batch_size]))
#%%

critic_params = {
    'layers': 2,
    'hidden_dim': 256,
    'activation': 'relu',
}

def MINE(scores):
    def marg(x):
        batch_size = x.shape[0]
        marg_ = tf.reduce_mean(tf.exp(x - tf.linalg.tensor_diag(np.inf * tf.ones(batch_size))))
        return marg_*((batch_size*batch_size)/(batch_size*(batch_size-1.)))
    joint_term = tf.reduce_mean(tf.linalg.diag_part(scores))
    marg_term = marg(scores)
    return joint_term - tf.math.log(marg_term)



# %%

noise_std = EbNo_to_noise(TRAINING_SNR)
noise_std_eve = EbNo_to_noise(7)

# custom functions / layers without weights
norm_layer = keras.layers.Lambda(lambda x: tf.divide(x,tf.sqrt(2*tf.reduce_mean(tf.square(x)))))
shape_layer = keras.layers.Lambda(lambda x: tf.reshape(x, shape=[-1,2,n]))
shape_layer2 = keras.layers.Lambda(lambda x: tf.reshape(x, shape=[-1,2*n]))
channel_layer = keras.layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std))
channel_layer_eve = keras.layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std_eve))


encoder = keras.models.Sequential([
            keras.layers.Embedding(M, M, embeddings_initializer='glorot_normal', input_length=1),
            keras.layers.Dense(M, activation="elu"),
            keras.layers.Dense(2*n, activation=None),
            shape_layer,
            norm_layer])

channel = keras.models.Sequential([channel_layer])
channel_eve = keras.models.Sequential([channel_layer, channel_layer_eve])

decoder_bob = keras.models.Sequential([
    keras.layers.InputLayer(input_shape=[2, n]),
    shape_layer2,
    keras.layers.Dense(M, activation="elu"),
    keras.layers.Dense(M, activation="softmax")
])

decoder_eve = keras.models.Sequential([
    keras.layers.InputLayer(input_shape=[2, n]),
    shape_layer2,
    keras.layers.Dense(M, activation="elu"),
    keras.layers.Dense(M, activation="softmax")
])

autoencoder_bob = keras.models.Sequential([encoder, channel, decoder_bob])
autoencoder_eve = keras.models.Sequential([encoder, channel_eve, decoder_eve])


# %%

def B_Ber(input_msg, msg):
    '''Calculate the Batch Bit Error Rate'''
    pred_error = tf.not_equal(tf.argmax(msg, 1), tf.argmax(input_msg, 1))
    bber = tf.reduce_mean(tf.cast(pred_error, tf.float32))
    return bber
#%%

def B_Ber_m(input_msg, msg):
    '''Calculate the Batch Bit Error Rate'''
    batch_size = input_msg.shape[0] #input_msg的行数
    pred_error = tf.not_equal(tf.reshape(input_msg, shape=(-1,batch_size)), tf.argmax(msg, 1))
    bber = tf.reduce_mean(tf.cast(pred_error, tf.float32))
    return bber

# %%

def random_sample(batch_size=32):
    msg = np.random.randint(M, size=(batch_size,1))
    return msg

#%%

def test_encoding(M=16, n=1):
    inp = np.arange(0,M)
    coding = encoder.predict(inp)  #(16,2,1）
    fig = plt.figure(figsize=(4,4))
    plt.plot(coding[:,0], coding[:, 1], "b.")
    plt.xlabel("$x_1$", fontsize=18)
    plt.ylabel("$x_2$", fontsize=18, rotation=0)
    plt.grid(True)
    plt.gca().set_ylim(-2, 2)
    plt.gca().set_xlim(-2, 2)
    plt.show()

#%%

def test_noisy_codeword(data):
    rcvd_word = data[1:2000]
    fig = plt.figure(figsize=(4,4))
    plt.plot(rcvd_word[:,0], rcvd_word[:, 1], "b.")
    plt.xlabel("$x_1$", fontsize=18)
    plt.ylabel("$x_2$", fontsize=18, rotation=0)
    plt.grid(True)
    plt.gca().set_ylim(-2, 2)
    plt.gca().set_xlim(-2, 2)
    plt.show()



# %%

loss_fn = keras.losses.SparseCategoricalCrossentropy()
mean_loss0 = keras.metrics.Mean()
mean_loss1 = keras.metrics.Mean()


# %%

def plot_loss(step, epoch, mean_loss0, X_batch, y_pred, plot_encoding):
    if step % 10 == 0:
        print('Iteration: '+str(step)+", Epoch: "+str(epoch)+"Loss: "+str(mean_loss0.result())+", Batch_BER: "+str(B_Ber_m(X_batch, y_pred)))
        if plot_encoding:
            test_encoding()


# %%

def plot_batch_loss(epoch, mean_loss0, X_batch, y_pred):
    ##print("Iter: "+str(epoch)+" ,Testing Accuracy "+str(acc))
    print("Interim result for Epoch: "+str(epoch)+", Loss: "+str(mean_loss0.result())+", Batch_BER: "+str(B_Ber_m(X_batch, y_pred)))

#%%

def train_mi(NN_estimation0, NN_estimation1,n_epochs=5, n_steps=500, batch_size=64, learning_rate=0.001):
    optimizer_mi0 = keras.optimizers.Nadam(lr=learning_rate)
    optimizer_mi1 = keras.optimizers.Nadam(lr=learning_rate)
    for epoch in range(1, n_epochs + 1):
        print("Training in Epoch {}/{}".format(epoch, n_epochs))
        for step in range(1, n_steps + 1):
            X_batch = random_sample(batch_size)
            with tf.GradientTape(persistent=True) as tape:
                x_enc = encoder(X_batch, training=False)
                #print(x_enc.shape)  #(64,2,1)
                y_recv = channel(x_enc)
                z_recv = channel_eve(x_enc)
                x = tf.reshape(x_enc, shape=[batch_size,2*n])
                y = tf.reshape(y_recv, shape=[batch_size,2*n])
                z = tf.reshape(z_recv, shape=[batch_size, 2 * n])
                score0 = NN_estimation0(x, y)
                score1 = NN_estimation1(x, z)
                loss0 = -MINE(score0)
                loss1 = -MINE(score1)
                gradients0 = tape.gradient(loss0, NN_estimation0.trainable_variables)
                optimizer_mi0.apply_gradients(zip(gradients0, NN_estimation0.trainable_variables))
                gradients1 = tape.gradient(loss1, NN_estimation1.trainable_variables)
                optimizer_mi1.apply_gradients(zip(gradients1, NN_estimation1.trainable_variables))
            mi_avg0 = -mean_loss0(loss0)
            mi_avg1 = -mean_loss1(loss1)
        print('Epoch: {}, Mi0 is {}， Mi1 is {}'.format(epoch, mi_avg0, mi_avg1))
        mean_loss0.reset_states()
        mean_loss1.reset_states()

#%%

def train_encoder(NN_estimation0, NN_estimation1, alpha=0.1, n_epochs=5, n_steps=400, batch_size=64, learning_rate=0.05):
    optimizer_mi0 = keras.optimizers.Nadam(lr=0.005)
    optimizer_mi1 = keras.optimizers.Nadam(lr=0.005)
    optimizer_ae = keras.optimizers.Nadam(lr=learning_rate)
    for epoch in range(1, n_epochs + 1):
        print("Training Bob in Epoch {}/{}".format(epoch, n_epochs))
        for step in range(1, n_steps + 1):
            X_batch  = random_sample(batch_size)
            with tf.GradientTape() as tape:
                x_enc = encoder(X_batch, training=True)
                # y_recv = tf.grad_pass_through(channel)(x_enc) #forward pass:channel; backward pass Identity
                # z_recv = tf.grad_pass_through(channel_eve)(x_enc)  # forward pass:channel; backward pass Identity
                y_recv = channel(x_enc)
                z_recv = channel_eve(x_enc)
                x = tf.reshape(x_enc, shape=[batch_size,2*n])
                y = tf.reshape(y_recv, shape=[batch_size,2*n])
                z = tf.reshape(z_recv, shape=[batch_size, 2 * n])
                score0 = NN_estimation0(x, y)
                score1 = NN_estimation1(x, z)
                T0=-MINE(score0)
                T1=-MINE(score1)
                T1_ = tf.abs(-T1)
                loss = alpha*T0+(1-alpha)*T1_  #loss = alpha*T0+(1-alpha)*T1_这样的损失函数是没有用的。
                gradients = tape.gradient(loss, encoder.trainable_variables)
                optimizer_ae.apply_gradients(zip(gradients, encoder.trainable_variables))
            mi_avg = -mean_loss0(loss)  #这里暂用alpha*mi0-(1-alpha)*mi1表示mi0-mi1
        with tf.GradientTape(persistent=True) as tape:
            X_batch = random_sample(batch_size)
            x_enc = encoder(X_batch, training=False)
            y_recv = channel(x_enc)
            z_recv = channel_eve(x_enc)
            x = tf.reshape(x_enc, shape=[batch_size, 2 * n])
            y = tf.reshape(y_recv, shape=[batch_size, 2 * n])
            z = tf.reshape(z_recv, shape=[batch_size, 2 * n])
            score0 = NN_estimation0(x, y)
            score1 = NN_estimation1(x, z)
            loss0 = -MINE(score0)
            loss1 = -MINE(score1)
            gradients0 = tape.gradient(loss0, NN_estimation0.trainable_variables)
            optimizer_mi0.apply_gradients(zip(gradients0, NN_estimation0.trainable_variables))
            gradients1 = tape.gradient(loss1, NN_estimation1.trainable_variables)
            optimizer_mi1.apply_gradients(zip(gradients1, NN_estimation1.trainable_variables))
        print('Epoch: {}, Mi0-Mi1 is {}'.format(epoch, mi_avg))

#%%

def train_decoder(n_epochs=5, n_steps=400, batch_size=500, learning_rate=0.005, plot_encoding=True):
    optimizer_ae = keras.optimizers.Nadam(lr=learning_rate)
    for epoch in range(1, n_epochs + 1):
        print("Training Bob in Epoch {}/{}".format(epoch, n_epochs))
        for step in range(1, n_steps + 1):
            X_batch  = random_sample(batch_size)
            with tf.GradientTape() as tape:
                y_pred = autoencoder_bob(X_batch, training=True)
                loss = tf.reduce_mean(loss_fn(X_batch, y_pred))
                gradients = tape.gradient(loss, decoder_bob.trainable_variables)
                optimizer_ae.apply_gradients(zip(gradients, decoder_bob.trainable_variables))
            mean_loss0(loss)
            plot_loss(step, epoch, mean_loss0, X_batch, y_pred, plot_encoding)
        plot_batch_loss(epoch, mean_loss0, X_batch, y_pred)
        mean_loss0.reset_states()
def train_Eve(n_epochs=5, n_steps=400, batch_size=500, learning_rate=0.005, plot_encoding=True):
    optimizer = keras.optimizers.Nadam(lr=0.005)
    for epoch in range(1, n_epochs + 1):
        print("Training Eve in Epoch {}/{}".format(epoch, n_epochs))
        for step in range(1, n_steps + 1):
            X_batch  = random_sample(batch_size)
            with tf.GradientTape() as tape:
                y_pred = autoencoder_eve(X_batch, training=True)
                main_loss = tf.reduce_mean(loss_fn(X_batch, y_pred))
                loss = main_loss
            gradients = tape.gradient(loss, decoder_eve.trainable_variables)
            optimizer.apply_gradients(zip(gradients, decoder_eve.trainable_variables))
            mean_loss0(loss)
            plot_loss(step, epoch, mean_loss0, X_batch, y_pred, plot_encoding)
        plot_batch_loss(epoch, mean_loss0, X_batch, y_pred)

# %%

def Test_AE():
    '''Calculate Bit Error for varying SNRs'''
    snr_range = np.linspace(0, 21, 31)
    bber_vec_bob = [None] * len(snr_range)
    bber_vec_eve = [None] * len(snr_range)

    for db in range(len(snr_range)):
        for it in range(1, 1000):
            noise_std = EbNo_to_noise(snr_range[db])
            noise_std_eve = EbNo_to_noise(7)
            X_batch = random_sample(500)
            code_word = encoder.predict(X_batch,steps=1)
            rcvd_word_bob = code_word + tf.random.normal(tf.shape(code_word), mean=0.0, stddev=noise_std)
            rcvd_word_eve = rcvd_word_bob + tf.random.normal(tf.shape(code_word), mean=0.0, stddev=noise_std_eve)
            dcoded_msg_bob = decoder_bob.predict(rcvd_word_bob,steps=1)
            dcoded_msg_eve = decoder_eve.predict(rcvd_word_eve,steps=1)
            bber_bob_bob = B_Ber_m(X_batch, dcoded_msg_bob)
            bber_bob_eve = B_Ber_m(X_batch, dcoded_msg_eve)
            bber_avg_bob = mean_loss0(bber_bob_bob)
            bber_avg_eve = mean_loss1(bber_bob_eve)
        bber_vec_bob[db] = bber_avg_bob
        bber_vec_eve[db] = bber_avg_eve
        mean_loss0.reset_states()
        mean_loss1.reset_states()
        if (db % 6 == 0) & (db > 0):
            print('Progress:', snr_range[db], 'bob_ber:', bber_vec_bob[db], 'eve_ber:', bber_vec_eve[db])
        # test_noisy_codeword(rcvd_word_bob)
        # test_noisy_codeword(rcvd_word_eve)
    return (snr_range, bber_vec_bob), (snr_range, bber_vec_eve)

# %%
score_fn0 = NN_function0(**critic_params)
score_fn1 = NN_function1(**critic_params)
train_mi(score_fn0, score_fn1, n_epochs=6, n_steps=500, batch_size=64, learning_rate=0.001)
train_encoder(score_fn0, score_fn1, alpha=0.7, n_epochs=8, n_steps=400, batch_size=64, learning_rate=0.001)
test_encoding(M, 1)
train_decoder(n_epochs=6, n_steps=400, batch_size=500, learning_rate=0.001, plot_encoding=False)
train_Eve(n_epochs=6, n_steps=400, batch_size=500, learning_rate=0.001, plot_encoding=False)

# %%
bber_data_bob, bber_data_eve = Test_AE()

#%%

# Approximate 16 QAM Error
def SIXT_QAM_sim(ebnodb):
    ebno = 10.**(ebnodb/10)
    return (3.0/2)*special.erfc(np.sqrt((4.0/10)*ebno))

def MQAM_rayleigh_approx(M, ebnodb):
    ebno = 10.**(ebnodb/10)
    esno = 4*ebno
    #Goldsmith, p.185, 6.3.2, Eqn 6.61, alphaM=4, betaM=3/(M-1)
    a=3.25 #adjusted mean number of neighbors
    b=3/(M-1)
    e=b*esno
    return (a/2)*(1-np.sqrt(0.5*e / (1+0.5*e))), a/(2*b*esno)

# %%
ebnodbs = np.linspace(0,15,16)
fig = plt.figure(figsize=(8, 5))
# bber_data_bob=[[0,5,10,15],[0.9,0.1,0.01,0.001]]
# bber_data_eve=[[0,5,10,15],[0.9,0.1,0.01,0.001]]
plt.semilogy(bber_data_bob[0], bber_data_bob[1], 's-',linewidth=2.0)
plt.semilogy(bber_data_eve[0], bber_data_eve[1], 'o-')
plt.semilogy(ebnodbs, SIXT_QAM_sim(ebnodbs), '^-')
plt.gca().set_ylim(1e-5, 1)
plt.gca().set_xlim(0, 21)
plt.tick_params(axis='x', colors='black')
plt.tick_params(axis='y', colors='black')
plt.ylabel("Batch Symbol Error Rate", fontsize=14, rotation=90, color='black')
plt.xlabel("SNR [dB]", fontsize=18, color='black')
plt.legend(['Bob','Eve','16QAM'], prop={'size': 8}, loc='upper right')
plt.grid(True, which="both")
#print(bber_data_bob[1][15],bber_data_eve[1][15])
# %%

